#
# Packages to install on all hosts. Most of these are for debug and convenience, none
# essential for AIS.
#
# XXX TODO Need to separate this into Linux flavours - these assume Ubuntu
#
ais_packages:
  - dstat
  - ethtool
  - iproute2
  - net-tools
  - sysstat
  - util-linux
  - lshw
  - util-linux
  - dmidecode
  - lsscsi
  - smartmontools
  - sdparm
  - iotop
  - procps
  - iftop
  - nicstat
  - linux-tools-common
  - linux-tools-{{ ansible_kernel }}
  - procps
  - sysstat
  - tcpdump
  - htop
  - atop
  - nmon
  - strace
  - bpfcc-tools
  - linux-headers-{{ ansible_kernel }}
  - systemtap
  - fio
  - iperf
  - attr
  - xfsprogs
  - vim
  - traceroute
  - curl
  - python
  - net-tools
  - jq
  - make
  - gcc
  - g++

#
# Packages unique to GPU systems. Don't include CUDA here - just generally-
# available Ubuntu packages. Some of these duplicate with list above - just 
# tracking the requirements from https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions
# These are packages for the host OS - so no need to include other DL software which
# will run in containers.
#
ais_gpu_packages:
  - gcc
  - linux-headers-{{ ansible_kernel }}

#
# CUDA and nvidia-docker install details - versions etc matched to host OS. We require just the drivers,
# not the CUDA runtime.
# XXX should be able to use ansible vars here; note that the repo servers are case-sensitive
#
cuda_repo_deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.105-1_amd64.deb
cuda_repo_key: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
nvidia_docker_key: 'https://nvidia.github.io/nvidia-docker/gpgkey'
libnvidia_container_key: 'https://nvidia.github.io/libnvidia-container/gpgkey'

#
# Caution is required  when blasting out host/dev combos for mkfs via ansible!
# The playbooks are intended to mkfs at initial cluster establishment, and also
# when new nodes are added (or disks replaced, or extra disks made available).
#
# To try to make it more difficult to mistakenly blast out a mkfs to all k8s nodes
# we choose not to list any host grouping here, and require that the intended
# set of hosts be specified on the playbook cmdline with -e, for example:
# 
# ansible-playbook -i hosts.ini ais_datafs_mkfs.yml \
#     -e '{"ais_hosts": ["cpu01"], "ais_devices": ["sda", "sdb"]}'
#
# You can also specify variables in a json or yaml file and use -e "@file.yml" etc
#
ais_hosts: dummy-to-require-cmdline-specification-of-hosts

#
# Same approach here. List on playbook cmdline with -e.
#
ais_devices:
        #- sda
        #- sdb
        #- sdc
        #- sdd
        #- sde
        #- sdf
        #- sdg
        #- sdh
        #- sdi
        #- sdj

#
# MTU for host NICs. XXX would be best to parametrize NIC name or driver
# Note that if changing this you also need to change the MTU used
# in Calico within k8s (to at least 20 bytes less than this value).
#
ais_host_mtu:
  - interface: enp94s0
    driver: mlx5_core
    mtu: 9000

#
# Lines to add to /etc/security/limits.conf (or /etc/security/limits.d/)
#
ais_pam_limits:
  - limit_item: nofile
    limit_type: soft
    value: 1048576
    comment: required in AIS docs (but also need to change in pods)
  - limit_item: nofile
    limit_type: hard
    value: 1048576
    comment: required in AIS docs (but also need to change in pods)

#
# CPU frequency governor selection
#
ais_cpufreq_governor: performance

#
# Block device tweaks suitable for xfs on scsi HDD
# See e.g. https://www.beegfs.io/wiki/StorageServerTuning
#
blkdevtune:
  scheduler: deadline
  nr_requests: 4096
  read_ahead: 4096

#
# Items to add/tweak in /etc/sysctl.conf. The 'default' is just for reference/comparison,
# as is the 'comment' field. These values assume a fat network of 50Gbit/s upwards.
# Validation has been on storage nodes with 50Gbit/s ethernet serving GPU nodes with
# 100Bgit/s ethernet all connected via the same top-of-rack switch.
#
ais_host_sysctl:
  - name: net.core.somaxconn
    value: 1024
    default: 128
    state: present
    comment: Maximum number of connection requests that can be queued to a given listening socket. Needs to absorb burst of connection requests. AIS clients will usually keep connections open to proxy and targets, so we do not expect ongoing high new connection rate.
  - name: net.core.rmem_max
    value: 268435456
    default: 212992
    state: present
    comment: Max receive socket buffer size for all protocols. AIS uses TCP, so net.ipv4.tcp_rmem over-rides there. Tweak to allow apps using SO_RCVBUF wide range. Will not apply to AIS.
  - name: net.core.wmem_max
    value: 268435456
    default: 212992
    state: present
    comment: Max send socket buffer size for all protocols. TCP over-rides, as above. Tweak to allow apps using SO_SNDBUF wide range. Will not apply to AIS.
  - name: net.core.rmem_default
    value: 25165824
    default: 212992
    state: present
    comment: Default  receive socket buffer size for all protocol. TCP over-rides etc as above. Does not apply to AIS.
  - name: net.core.wmem_default
    value: 25165824
    default: 212992
    state: present
    comment: Default send socket buffer size for all protocol. TCP over-rides etc as above. Does not apply to AIS.
  - name: net.core.optmem_max
    value: 25165824
    default: 20480
    state: present
    comment: Max ancillary buffer size. Some say must tune up, some say no difference.
  - name: net.core.netdev_max_backlog
    value: 250000
    default: 1000
    state: present
    comment: Maximum number of packets queued on input side if kernel is unable to process packet fast enough to keep up with receive rate. Value per Mellanox tuning guide.
  - name: net.ipv4.tcp_wmem
    value:   4096    12582912  268435456
    default: 4096       16384  4194304 (max is calculated)
    state: present
    comment: Min guaranteed under memory pressure, default, and maximum TCP send socket size. Applies only to automatically-sized buffers (as used in AIS) - apps that set SO_SNDBUF get what they request. Allow automatic sizing up to 128M.
  - name: net.ipv4.tcp_rmem
    value:   4096    12582912 268435456
    default: 4096       87380  6291456 (max is calculated)
    state: present
    comment: Min guaranteed under memory pressure, default, and maximum TCP receive socket size. Applies only to automatically-sized buffers (as used in AIS) - apps that set SO_RCVBUF get what they request. Allow automatic sizing up to 128M.
  - name: net.ipv4.tcp_adv_win_scale
    value: 1
    default: 2
    state: present
    comment: Advised by Melanox docs (https://community.mellanox.com/s/article/linux-sysctl-tuning); share socket buffer equally between app and kernel, default is app gets 1/4.
  - name: net.ipv4.tcp_mtu_probing
    value: 2
    default: 0
    state: present
    comment: We control the MTU within the cluster nodes and k8s pods, so we can always use an initial MSS of tcp_base_mss.
  - name: net.ipv4.tcp_slow_start_after_idle
    value: 0
    default: 1
    state: present
    comment: No RFC2861 decay of the congestion window when all on same fast LAN
  - name: net.ipv4.tcp_tw_reuse
    value: 1
    default: 0
    state: present
    comment: If sockets hang around in timewait state for long then (since we're PUTting and GETting lots of objects) we very soon find that we exhaust local port range. So we stretch the available range of local ports (ip_local_port_range), increase the max number of timewait buckets held by the system simultaneously (tcp_max_tw_buckets), and reuse sockets in timewait state as soon as it is "safe from a protocol point of view" (whatever that means, tcp_tw_reuse).
  - name: net.ipv4.ip_local_port_range
    value:   10240 65535 
    default: 32768 60999 
    state: present
    comment: See comment for tw_reuse
  - name: net.ipv4.tcp_max_tw_buckets
    value:   1440000
    default:  262144
    state: present
    comment: See comment for tw_reuse
  - name: net.ipv4.tcp_low_latency
    value: 1
    default: 0
    state: present
    comment: Per Mellanox tuning guide
  - name: net.ipv4.tcp_timestamps
    value: 0
    state: present
    default: 1
    comment: Per Mellanox tuning guide