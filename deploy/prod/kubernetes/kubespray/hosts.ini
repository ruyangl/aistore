#
# A manually curated inventory list seems to suit us nicely. With a fairly
# static cluster size we have no need for a dynamic inventory, and the
# inventory generation tool in kubespray is inflexible.
#
# Kubespray requires that we define the following groups (see docs/ansible.md):
#
# - kube-master, covering masters (can be on distinct hosts or co-resident
#   with a worker node
# - etcd, covering etcd nodes, again separate or co-resident; minimum of 3,
#   odd number mandated
# - kube-node, covering all worker nodes
# - k8s-cluster, covering worker nodes and masters
# - calico-rr - XXX
#
# Kubespray also has playbooks for adding and removing non-master nodes.
# When a node is removed it should be removed from the list of worker nodes
# here, but it is still convenient to have *some* entry for it here unless
# we are retiring the node permanently.
#
# We also have two distinct types of worker node - CPU (aka Storage) nodes
# and DGX (GPU) nodes.
#
# So we will build the list required bu Kubespray from a few base lists:
#
# - cpu-node-population, defines all CPU nodes available to us even if some
#   are not included in the cluster
# - gpu-node-population, similarly, for GPU nodes
# - cpu-worker-node, active CPU worker nodes (subset of population)
# - gpu-worker-node, active GPU worker nodes (subset of population)

#
# All CPU nodes we want to talk to with Ansible, regardless of whether
# included/active in cluster.
#
[cpu-node-population]
<REPLACE_CPU_NODES>
#Example format
#cpu01 ansible_host=<ip> ip=<ip> ansible_user=<user>


#
# All GPU nodes we want to talk to with Ansible, regardless of whether
# included/active in cluster.
#
[dgx-node-population]
<REPLACE_DGX_NODES>
#dgx01 ansible_host=<ip> ip=<ip> ansible_user=<user>

#
# Active CPU worker nodes
#
[cpu-worker-node]
<REPLACE_CPU_WORKER_NODES>
#Example format:
#cpu01

#
# Active GPU worker nodes
#
[gpu-worker-node]
<REPLACE_DGX_WORKER_NODES>
#Example format:
#dgx01

#
# Kube master hosts
#
[kube-master]
<REPLACE_MASTER_LIST>
#Example format:
#cpu01

#
# The etcd cluster hosts
#
[etcd]
<REPLACE_ETCD_LIST>
#Example format:
#cpu01

#
# kube-node addresses all worker nodes
#
[kube-node:children]
cpu-worker-node
gpu-worker-node

#
# k8s-cluster addresses the worker nodes and the masters
#
[k8s-cluster:children]
kube-master
kube-node

#
# All nodes - not required by kubespray, so only for admin convenience.
# Loops in active workers of all types, etcd and master hosts.
#
# XXX Tempting to name this ‘all’, but Ansible seems to expand that to
# mean “all hosts mentioned in the inventory regardless of grouping”.
#
[allactive:children]
k8s-cluster
etcd

#
# See kubespray docs/ansible.md
#
[calico-rr]

#If kubespray needs to hop over a bastion host to configure the k8s hosts
#include the below config
[bastion]
bastion ansible_host=<REPLACE_BASTION_IP> ansible_user=root


